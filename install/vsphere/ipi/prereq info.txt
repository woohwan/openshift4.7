Local DNS: 172.20.2.230
DNS: 172.20.2.231

Bastion: 172.20.2.163
Router: 172.20.2.235
mirror registry: 172.20.2.226
NFS: 172.20.220.2.236

api vip: 172.20.2.220  --> 192.168.1.201
ingress vip: 172.20.2.221 --> 192.168.1.202

1. DNS for OCP
   ip: 172.20.2.231
   for DNS: 127.0.0.1 (?)
   ensure that Ignore automatically Obtained DNS parameters is ticked
    
   * /var/named/* 에 새로 생성한 zone 파일의 소유권 변경: chwon :named *.zone, *.rev

  ** provision network에서 vCenter 접속 가능해야 함. ==> 자동설치 시 vm을 생성하는 것이 vCenter임
   ------------------------------------------------
   * 하나의 nameserver 에 multi domain을 설정하는 방법
   -------------------------------------------------
   It is only possible to use a single zone file for multiple domains, 
   if all the domains share the same DNS entries, and especially the same IP addresses.

   As an example :

  zone "domain1.com" {
    file "mydomain.com.zone";
  };

  zone "domain2.com" {
    file "mydomain.com.zone";
  };

  Some rules for the contents of the shared file:

  Don't include a $ORIGIN statement - it's implicit from the config file
  Use '@' to refer to the implicit $ORIGIN
  Use relative domain names (not FQDNs) as appropriate.
  Use FQDNs when it actually matters which domain is returned
  An example for this file would be:

  @      IN      SOA      data
         IN      NS       ns.example.com.
  mail   IN      MX       mail.example.com.
  web    IN      A        1.2.3.4
  www    IN      CNAME    web
  ftp    IN      CNAME    web
--------------------------------------------------------------------------------   

2. GW for OCP Network 
  - external ip: 172.20.2.235
  - internal ip: 192.168.1.1
  - ensure that Never use this network for default route is ticked

  $ nmcli con add type ethernet con-name ens192 ifname ens192
  $ nmcli con mod ens192 ipv4.addresses 192.168.1.1 \
		ipv4.gateway 192.168.1.1 \
		ipv4.dns 172.20.2.231 \
		ipv4.metho manual connection.autoconnect yes
  $ firewall-cmd --get-active-zone
  $ firewall-cmd --get-zones
  $ firewall-cmd --get-default-zone
  $ firewall-cmd --set-default-zone external
  $ nmcli con modify ens192 connection.zone external
  $ nmcli con mod ens224 connection.zone internal
  $ firewall-cmd --zone=external --add-masquerade --permanent
  $ firewall-cmd --zone=internal --add-masquerade --permanent
  $ firewall-cmd --reload
  $ cat /proc/sys/net/ipv4/ip_forward
  
  # priavate (192.168.1.0/24)과 외부와 통신하기 위해서 (참고용: 적용하지 않아도 작동함)

  아래 것은 interface를 통한 port-frowarding으로 작동하지 않음. (검증 필요)
  $ firewall-cmd --direct --add-rule ipv4 filter FORWARD 0 -i ens224 -o ens192 -j ACCEPT # internal --> external
  $ firewall-cmd --direct --add-rule ipv4 filter FORWARD 0 -i ens192 -o ens224 -m state --state NEW,RELATED,ESTABLISHED -j ACCEPT

  ** external에서 internal의 haproxy로 연결하기 위해서는 virtual ip  (혹은 secondary ip) 및 port-forwarding이 필요
    --> haproxy를 위한 가상 vip  
      172.20.2.221 (API VIP) --> 192.168.1.201 
      172.20.2.222 (Ingress VIP) --> 192.168.1.202

  2.1 Interace에 multi IP를 구성하는 방법 3가지
    2.1.1 Interface에 직접 Secondary IP 추가
    $ /etc/sysconfig/network-scripts/ifcfg-ens192 파일 수정
    IPADDR=172.20.2.235    --> IPADDR0=172.20.2.221
                               IPADDR1=172.20.2.222
    $ifdown ens192 && ifup ens192
    $ ip a show ens192

    2.2.2 virtual ip 추가
    - ifcfg-ens192 파일을 복사 및 수정
    $ cd /etc/sysconfig/network-scripts/
    $ cp ifcfg-ens192 ifcfg-ens192:1
    $ cat ifcfg-ens192:1
    TYPE=Ethernet
    DEVICE=ens192:1
    ONBOOT=yes
    NM_CONTROLLED=no
    BOOTPROTO=none
    IPADDR=172.20.2.221
    PREFIX=22
    GATEWAY=172.20.0.1
    DNS1=172.20.2.231
    PEERDNS=no
    ZONE=external

    [root@router-ocp network-scripts]# cat ifcfg-ens192:2
    TYPE=Ethernet
    DEVICE=ens192:2
    ONBOOT=yes
    NM_CONTROLLED=no
    BOOTPROTO=none
    IPADDR=172.20.2.222
    PREFIX=22
    GATEWAY=172.20.0.1
    DNS1=172.20.2.231
    PEERDNS=no
    ZONE=external

    $ ifdown ens192 && ifup ens192
    $ ip a show ens192

    2.2.3 nmcli를 이용한 secondary ip 추가
    $ nmcli con mod ens192 +ipv4.addresses "172.20.2.221/22"
    $ nmcli con mod ens192 +ipv4.addresses "172.20.2.222/22"
    $ nmcli con down ens192 && nmcli con up ens192
    $ ip a show ens192

  2.2 port-forwarding  추가 ( 참고: https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=alice_k106&logNo=221305928714)
    ** PREROUTING만 설정하면 됨.
    2.2.1 API VIP (172.20.2.220) 에 대한 nat 테이블에 PREROUTING 추가
      $ iptables -t nat -A PREROUTING -d 172.20.2.220 -p tcp --dport 80 -j DNAT --to-destination 192.168.1.201:80
      설명)
      · - A PREROUTING: PREROUTING 체인에 규칙을 추가한다.
      · - t nat: nat 테이블에 규칙을 추가한다.
      · -j DANT: 사용할 기능으로 DNAT, SNAT, MAQUERADE 설정. 여기서는 외부에서 들어오는 패킷의 destination 주소를 변경하므로 DNAT
      · -p tcp: tcp 프로토콜을 사용한다.
      · --dport: 들어오는 패킷의 목적지 포트를 명시한다.
      · --to-destination: 최종적으로 DANT에 의해 설정될 도착지를 설정한다.

      $ iptables -nL PREROUTING -t nat
      Chain PREROUTING (policy ACCEPT)
      target     prot opt source               destination
      DNAT       tcp  --  0.0.0.0/0            172.20.2.220         tcp dpt:80 to:192.168.1.201

     - Ingress VIP (172.20.2.221) 에 대해서도 동일하게 수행 
    $ iptables -t nat -A PREROUTING -d 172.20.2.118 -p tcp --dport 443 -j DNAT --to-destination 192.168.1.202:443
    --> firewall-cmd --reload 하면 설정 값이 사라짐.
    $ firewall-cmd  --permanent --direct --add-rule ipv4 nat PREROUTING 0 -d 172.20.2.118 -p tcp --dport 443 -j DNAT --to-destination 192.168.1.202:443
    $ firewall-cmd --reload

    * 잘못 입력해서 --reload가 적용되지 않을 경우, /etc/firewalld/direct.xml 파일 수정

    =================================== port forwarding in router - 2021.11.30 ======================================================
    $ firewall-cmd --zone=external --list-all
    external (active)
    target: default
    icmp-block-inversion: no
    interfaces: ens192
    sources:
    services: http https ssh
    ports:
    protocols:
    forward: no
    masquerade: yes
    forward-ports:
    source-ports:
    icmp-blocks:
    rich rules:

    $ firewall-cmd --zone=internal --list-all
      internal (active)
      target: default
      icmp-block-inversion: no
      interfaces: ens224
      sources:
      services: cockpit dhcpv6-client mdns samba-client ssh
      ports:
      protocols:
      forward: no
      masquerade: yes
      forward-ports:
      source-ports:
      icmp-blocks:
      rich rules:
    ------------------------------------------------------------------------------------
    ** netfilter table
    $ iptables -nL
    Chain INPUT (policy ACCEPT)
    target     prot opt source               destination

    Chain FORWARD (policy ACCEPT)
    target     prot opt source               destination

    Chain OUTPUT (policy ACCEPT)
    target     prot opt source

    $ iptables -nL -t nat
    Chain PREROUTING (policy ACCEPT)
    target     prot opt source               destination
    DNAT       tcp  --  0.0.0.0/0            172.20.2.118         tcp dpt:443 to:192.168.1.202:443


    ============================









3. DHCP in OCP Network
  - ip : 192.168.1.2
  - gateway: 192.168.1.1
  - hostname: dhcp-ocp.saltware.lan
  - dns: 172.20.2.231
  
시스템 상에 한 개 이상의 네트워크 인터페이스가 연결되어 있는 경우, 오직 한 인터페이스 상에서만 DHCP 서버가 시작하도록 설정하는 것이 가능합니다. 
/etc/sysconfig/dhcpd 파일에서 해당 인터페이스의 이름을 DHCPDARGS 목록에 추가
-------------------- dhcpd.conf -----
authoritve;
ddns-update-style interim;
allow booting;
allow bootp;
allow unknown-clients;
ignore client-updates;
default-lease-time 14400;
max-lease-time 14400;


DHCPDARGS=ens192
subnet 192.168.1.0 netmask 255.255.255.0 {
 option routers                  192.168.1.1; # lan
 option subnet-mask              255.255.255.0;
 option domain-name              "saltware.lan";
 option domain-name-servers       172.20.2.231;
 range 192.168.1.10 192.168.1.200;
}
------------------------------------
4. downlaod  ocp client & installer and setup


4. Add the vCenter root CA certificates to your system trust
wget https://172.20.2.240/certs/download.zip --no-check-certificate
unzip download.zip
cp certs/lin/* /etc/pki/ca-trust/source/anchors
update-ca-trust extract


5. 
 - generate ssh key and start agent
 $ ssh-keygen
 $ eval "$(ssh-agent)"
 # add ssh private key to the ssh-agent


 6. client 설치
  - enable tab completion
  $ oc completion bash > oc_bash_completion
  $ sudo cp oc_bash_completion /etc/bash_completion.d/



8. Image Registry 생성
INI 설치 시 기본적으로 Image Registry는 제거된 상태로 설치가 완성된다. 
** 주의사항: vSphere에 INI 설치 시  내장 pvc가 default thin Storage Class를 사용하는 이는 RWX 모드를 지원하지 않는다.
따라서, Kubernetes NFS Subdir External Provisioner  ( https://github.com/woohwan/nfs-subdir-external-provisioner.git )를 먼저 설치/테스트 테스티한다.
  
  8.1 project 생성 ==> 모든 수행은 이 project 내에서....
  $ oc new-proejct nfs-provisioner

  8.2 권한 생성
  # Set the subject of the RBAC objects to the current namespace where the provisioner is being deployed
  $ NAMESPACE=`oc project -q`
  $ sed -i'' "s/namespace:.*/namespace: $NAMESPACE/g" ./deploy/rbac.yaml ./deploy/deployment.yaml
  $ oc create -f deploy/rbac.yaml
  $ oc adm policy add-scc-to-user hostmount-anyuid system:serviceaccount:$NAMESPACE:nfs-client-provisioner

  8.3 NFS subdir external provisioner 구성 및 수행
    8.3.1 ./deploy/deployment.yaml 을 구성한 NFS 정보에 맞게 수정
  -------------------------------------------
  apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: nfs-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 172.20.2.236
            - name: NFS_PATH
              value: /shares/registry
      volumes:
        - name: nfs-client-root
          nfs:
            server: 172.20.2.236
            path: /shares/registry
--------------------------------------------------------------------------
    8.3.2 
    $ oc create -f deploy/deployment.yaml
    $ oc get pods
    NAME                                     READY   STATUS    RESTARTS   AGE
    nfs-client-provisioner-6c7f447cd-jmg28   1/1     Running   0          10s

  8.4 Storage class 수행 및 테스트
    $ oc create -f deploy/class.yaml
    $ kubectl create -f deploy/test-claim.yaml -f deploy/test-pod.yaml

  8.5 Local Image Registry 용  PVC 생성
    $ oc proejct openshift-image-registry (namespace를 openshift-image-registry로 변경)
    $ vi image-registry-pvc.yaml
    --------------------------------------------
    kind: PersistentVolumeClaim
    apiVersion: v1
    metadata:
      name: image-registry-pvc
      annotations:
        nfs.io/storage-path: "/shares/registry" # not required, depending on whether this annotation was shown in the storage class description
    spec:
      storageClassName: managed-nfs-storage
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 100Gi
    --------------------------------------------------------
    $ oc get pvc
    NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
    image-registry-pvc   Bound    pvc-6fb2cd10-c7f8-4cd1-9950-3e96c13c382e   100Gi      RWX            managed-nfs-storage   6s

  8.6 image registry config 변경
    $ oc edit configs.imageregistry.operator.openshift.io -o yaml
    managementState: Removed --> Managed
    storage: {}

    storage:
      pvc:
        claim: image-registry-pvc

  8.7 mage-registry cluster-operator의 상태를 확인
    $ watch oc get co image-registry
    - Available이 True로 변경될때까지 기다립니다.
    pod 정상 작동 확인
    $ oc get po -n openshift-image-registry


